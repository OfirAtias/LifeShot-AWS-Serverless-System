#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LifeShot Auth Stack Bootstrap (Boto3)
- Creates Cognito User Pool + App Client (no secret)
- Creates Groups: Admins, Lifeguards
- Creates 2 users + assigns to groups
- Deploys Node Lambda: LifeShot_Login (LOGIN.MJS autogenerated + npm install + zip)
- Creates/Ensures HTTP API (API Gateway v2 HTTP) with routes:
    /auth/login             POST   (NONE)
    /auth/complete-password POST   (NONE)
    /auth/me                GET    (NONE)
    /auth/logout            POST   (NONE)
    /events                 GET    (JWT Auth)   -> Events API Lambda (LifeShot_Api_Handler)
    /events                 PATCH  (JWT Auth)   -> Events API Lambda (LifeShot_Api_Handler)
- CORS for API Gateway: AllowOrigins ["*"], AllowHeaders ["authorization","content-type"],
  AllowMethods ["GET","POST","PATCH","OPTIONS"], AllowCredentials false
- Ensures Detector Lambda has a Function URL (Auth NONE) + CORS (POST/OPTIONS)
- SNS Topic + Access Policy + Email subscription
- Split Detector into 3 Lambdas (Detector / RenderAndS3 / EventsAndSNS)
    - Detector env vars: RENDER_LAMBDA_NAME + EVENTS_LAMBDA_NAME (points to EventsAndSNS)
    - EventsAndSNS env vars: SNS_TOPIC_ARN
    - Detector does NOT store SNS_TOPIC_ARN

NEW IN THIS VERSION 
- S3 bucket AUTO handling:
  - Tries default bucket lifeshot-pool-images (or FRAMES_BUCKET)
  - If bucket exists but is NOT writeable (AccessDenied on PutObject), or doesn't exist:
    -> auto-creates a NEW unique bucket in your account, and uses it.
- Ensures S3 prefixes (folder markers) (best-effort)
- Uploads local images into S3 automatically
- Creates/Ensures DynamoDB table
- Sets env vars on Lambdas (merge-safe): FRAMES_BUCKET, FRAMES_PREFIX, EVENTS_TABLE_NAME

Requirements:
- python3 + boto3 installed
- node + npm installed (only if login.zip not present)
"""

import base64
import json
import mimetypes
import os
import shutil
import subprocess
import sys
import tempfile
import time
import zipfile
import uuid
import re
from dataclasses import dataclass
from typing import Dict, Optional, Tuple, Any, List

import boto3
from botocore.exceptions import ClientError


# -------------------------
# Config (env overrides)
# -------------------------
REGION = os.getenv("AWS_REGION", "us-east-1")
STACK_PREFIX = os.getenv("STACK_PREFIX", "LifeShot")

USER_POOL_NAME = os.getenv("USER_POOL_NAME", f"{STACK_PREFIX}_UserPool")
APP_CLIENT_NAME = os.getenv("APP_CLIENT_NAME", "lifeshot-spa")

LOGIN_LAMBDA_NAME = os.getenv("LOGIN_LAMBDA_NAME", f"{STACK_PREFIX}_Login")

# Existing "API events" lambda used by API Gateway /events routes:
EVENTS_API_LAMBDA_NAME = os.getenv("EVENTS_API_LAMBDA_NAME", "LifeShot_Api_Handler")

# Split lambdas:
DETECTOR_LAMBDA_NAME = os.getenv("DETECTOR_LAMBDA_NAME", "LifeShot_detector_logic")
RENDER_LAMBDA_NAME = os.getenv("RENDER_LAMBDA_NAME", "LifeShot_RenderAndS3")
EVENTS_SNS_LAMBDA_NAME = os.getenv("EVENTS_SNS_LAMBDA_NAME", "LifeShot_EventsAndSNS")

# Handlers (because you renamed files - not lambda_function.py)
DETECTOR_HANDLER = os.getenv("DETECTOR_HANDLER", "detector_logic.lambda_handler")
RENDER_HANDLER = os.getenv("RENDER_HANDLER", "render_and_s3.lambda_handler")
EVENTS_SNS_HANDLER = os.getenv("EVENTS_SNS_HANDLER", "events_and_sns.lambda_handler")

# Memory requirements
DETECTOR_MEMORY = int(os.getenv("DETECTOR_MEMORY", "1024"))
RENDER_MEMORY = int(os.getenv("RENDER_MEMORY", "1024"))
EVENTS_SNS_MEMORY = int(os.getenv("EVENTS_SNS_MEMORY", "256"))

# Timeouts
DETECTOR_TIMEOUT = int(os.getenv("DETECTOR_TIMEOUT", "60"))
RENDER_TIMEOUT = int(os.getenv("RENDER_TIMEOUT", "45"))
EVENTS_SNS_TIMEOUT = int(os.getenv("EVENTS_SNS_TIMEOUT", "15"))

API_NAME = os.getenv("API_NAME", f"{STACK_PREFIX}HttpApi")

ALLOWED_ORIGIN = os.getenv("ALLOWED_ORIGIN", "http://localhost:5500")
ALLOWED_ORIGIN_ENV_SET = "ALLOWED_ORIGIN" in os.environ

# Users to create
ADMIN_EMAIL = os.getenv("ADMIN_EMAIL", "lifeguard647@gmail.com")
GUARD_EMAIL = os.getenv("GUARD_EMAIL", "lifeguarduser1@gmail.com")
TEMP_PASSWORD = os.getenv("TEMP_PASSWORD", "LifeShot!123")

GROUP_ADMINS = "Admins"
GROUP_LIFEGUARDS = "Lifeguards"

# SNS
SNS_TOPIC_NAME = os.getenv("SNS_TOPIC_NAME", "LifeShot-Drowning-Alerts")

# LabRole (default)
LAB_ROLE_NAME = os.getenv("LAB_ROLE_NAME", "LabRole")

# Runtime defaults
LOGIN_RUNTIME = os.getenv("LOGIN_RUNTIME", "nodejs20.x")
DEFAULT_PY_RUNTIME = os.getenv("DEFAULT_PY_RUNTIME", "python3.11")

# -------------------------
# S3 + DynamoDB data-plane config
# -------------------------
FRAMES_BUCKET = os.getenv("FRAMES_BUCKET", "lifeshot-pool-images")
FRAMES_PREFIX = os.getenv("FRAMES_PREFIX", "LifeShot/DrowningSet")

DATASET_TEST1 = os.getenv("DATASET_TEST1", "Test1")
DATASET_TEST2 = os.getenv("DATASET_TEST2", "Test2")

EVENTS_TABLE_NAME = os.getenv("EVENTS_TABLE_NAME", "LifeShot_Events")


# -------------------------
# Frontend (S3 static website)
# -------------------------
FRONTEND_BUCKET_NAME = os.getenv("FRONTEND_BUCKET_NAME",f"lifeshotweb-{uuid.uuid4().hex[:6]}")
FRONTEND_DIR = os.getenv(
  "FRONTEND_DIR",
  os.path.join(os.path.dirname(os.path.abspath(__file__)), "client")
)


# CORS for the frontend bucket itself (not API Gateway)
FRONTEND_CORS_ALLOWED_ORIGINS = os.getenv("FRONTEND_CORS_ALLOWED_ORIGINS", "*")


# -------------------------
# Helpers
# -------------------------
def retry_on_conflict(fn, *, retries: int = 12, delay_sec: int = 3):
    last = None
    for _ in range(retries):
        try:
            return fn()
        except ClientError as e:
            code = e.response.get("Error", {}).get("Code", "")
            if code == "ResourceConflictException":
                last = e
                time.sleep(delay_sec)
                continue
            raise
    raise last


def log(msg: str) -> None:
    print(f"[{time.strftime('%H:%M:%S')}] {msg}")


def die(msg: str) -> None:
    print(f"ERROR: {msg}", file=sys.stderr)
    sys.exit(1)


def need_cmd(cmd: str) -> None:
    if shutil.which(cmd) is None:
        die(f"Missing command on PATH: {cmd}")


def b64_zip_bytes(path: str) -> bytes:
    with open(path, "rb") as f:
        return f.read()


def safe_client(service: str):
    return boto3.client(service, region_name=REGION)


def get_account_id(sts) -> str:
    return sts.get_caller_identity()["Account"]


def wait_lambda_active(client_lambda, fn_name: str, timeout_sec: int = 180) -> None:
    """
    Wait until a Lambda function is Active. Prevents ResourceConflictException
    when updating config/env right after create/update.
    """
    try:
        waiter = client_lambda.get_waiter("function_active")
        waiter.wait(
            FunctionName=fn_name,
            WaiterConfig={"Delay": 3, "MaxAttempts": max(1, timeout_sec // 3)},
        )
        return
    except Exception:
        t0 = time.time()
        while time.time() - t0 < timeout_sec:
            try:
                cfg = client_lambda.get_function_configuration(FunctionName=fn_name)
                if cfg.get("State") == "Active":
                    return
            except ClientError:
                pass
            time.sleep(3)
        raise RuntimeError(f"Lambda did not become Active in time: {fn_name}")


def ensure_lambda_permission_invoke(
    client_lambda,
    fn_name: str,
    statement_id: str,
    principal: str,
    action: str,
    source_arn: Optional[str] = None,
    function_url_auth_type: Optional[str] = None,
) -> None:
    kwargs = {
        "FunctionName": fn_name,
        "StatementId": statement_id,
        "Action": action,
        "Principal": principal,
    }
    if source_arn:
        kwargs["SourceArn"] = source_arn
    if function_url_auth_type:
        kwargs["FunctionUrlAuthType"] = function_url_auth_type

    def _call():
        return client_lambda.add_permission(**kwargs)

    try:
        retry_on_conflict(_call)
        log(f"Added lambda permission: {fn_name} {statement_id}")
    except ClientError as e:
        code = e.response.get("Error", {}).get("Code", "")
        msg = e.response.get("Error", {}).get("Message", "") or str(e)

        if code in ("ResourceConflictException", "EntityAlreadyExistsException"):
            if "already exists" in msg.lower() or "statement id" in msg.lower():
                log(f"Lambda permission already exists: {fn_name} {statement_id}")
                return
        raise


def merge_env(existing: Optional[Dict[str, str]], updates: Dict[str, Optional[str]]) -> Dict[str, str]:
    cur = dict(existing or {})
    for k, v in updates.items():
        if v is None:
            cur.pop(k, None)
        else:
            cur[k] = v
    return cur


def zip_from_dir(src_dir: str, out_zip: str) -> None:
    with zipfile.ZipFile(out_zip, "w", compression=zipfile.ZIP_DEFLATED) as z:
        for root, _, files in os.walk(src_dir):
            for f in files:
                full = os.path.join(root, f)
                rel = os.path.relpath(full, src_dir)
                z.write(full, rel)


def now_utc_iso() -> str:
    return time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())


# -------------------------
# NEW: Frontend S3 static website hosting deploy
# -------------------------
def _parse_csv_env(value: str) -> List[str]:
    items = [x.strip() for x in str(value or "").split(",")]
    return [x for x in items if x]


def ensure_frontend_bucket_exists(s3, bucket: str, region: str) -> None:
    try:
        s3.head_bucket(Bucket=bucket)
        log(f"Frontend bucket exists: {bucket}")
        return
    except ClientError as e:
        code = e.response.get("Error", {}).get("Code", "")
        if code not in ("404", "NoSuchBucket", "NotFound"):
            # Could be 403 if bucket exists in another account
            raise

    log(f"Creating frontend bucket: {bucket}")
    if region == "us-east-1":
        s3.create_bucket(Bucket=bucket)
    else:
        s3.create_bucket(
            Bucket=bucket,
            CreateBucketConfiguration={"LocationConstraint": region},
        )
    time.sleep(2)


def ensure_frontend_public_access(s3, bucket: str) -> None:
    # Disable Block Public Access (all flags false)
    s3.put_public_access_block(
        Bucket=bucket,
        PublicAccessBlockConfiguration={
            "BlockPublicAcls": False,
            "IgnorePublicAcls": False,
            "BlockPublicPolicy": False,
            "RestrictPublicBuckets": False,
        },
    )

    # Public read bucket policy
    policy = {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Sid": "PublicReadGetObject",
                "Effect": "Allow",
                "Principal": "*",
                "Action": ["s3:GetObject"],
                "Resource": [f"arn:aws:s3:::{bucket}/*"],
            }
        ],
    }
    s3.put_bucket_policy(Bucket=bucket, Policy=json.dumps(policy))


def ensure_frontend_website_hosting(s3, bucket: str) -> None:
    s3.put_bucket_website(
        Bucket=bucket,
        WebsiteConfiguration={
            "IndexDocument": {"Suffix": "index.html"},
            "ErrorDocument": {"Key": "index.html"},
        },
    )


def ensure_frontend_bucket_cors(s3, bucket: str, allowed_origins: List[str]) -> None:
    s3.put_bucket_cors(
        Bucket=bucket,
        CORSConfiguration={
            "CORSRules": [
                {
                    "AllowedOrigins": allowed_origins,
                    "AllowedMethods": ["GET", "HEAD"],
                    "AllowedHeaders": ["*"],
                    "MaxAgeSeconds": 0,
                }
            ]
        },
    )


def _content_type_for_file(path: str) -> str:
    # Ensure common types are correct across OSes
    mimetypes.add_type("application/javascript", ".js")
    mimetypes.add_type("application/javascript", ".mjs")
    mimetypes.add_type("text/css", ".css")
    mimetypes.add_type("text/html", ".html")

    ctype, _ = mimetypes.guess_type(path)
    return ctype or "application/octet-stream"


def upload_frontend_dir(s3, bucket: str, local_dir: str) -> int:
    if not os.path.isdir(local_dir):
        raise RuntimeError(f"Frontend dir not found: {local_dir}")

    uploaded = 0
    for root, _, files in os.walk(local_dir):
        for f in files:
            local_path = os.path.join(root, f)
            rel_path = os.path.relpath(local_path, local_dir)
            key = rel_path.replace("\\", "/")

            extra = {"ContentType": _content_type_for_file(local_path)}
            s3.upload_file(local_path, bucket, key, ExtraArgs=extra)
            uploaded += 1
    return uploaded


def put_frontend_config_js(s3, bucket: str, api_base_url: str, detector_lambda_url: str) -> None:
    # This file is loaded by HTML pages to set window.API_BASE_URL, etc.
    body = (
        "// Auto-generated by lifeshot_bootstrap.py (do not edit manually)\n"
        f"window.API_BASE_URL = {json.dumps(api_base_url)};\n"
        f"window.AUTH_BASE_URL = {json.dumps(api_base_url)};\n"
        f"window.DETECTOR_LAMBDA_URL = {json.dumps(detector_lambda_url)};\n"
    ).encode("utf-8")

    s3.put_object(
        Bucket=bucket,
        Key="config.js",
        Body=body,
        ContentType="application/javascript; charset=utf-8",
        CacheControl="no-cache",
    )


def deploy_frontend_static_website(
    s3,
    *,
    bucket: str,
    region: str,
    local_dir: str,
    cors_allowed_origins: List[str],
    api_base_url: str,
    detector_lambda_url: str,
) -> str:
    ensure_frontend_bucket_exists(s3, bucket, region)
    ensure_frontend_public_access(s3, bucket)
    ensure_frontend_website_hosting(s3, bucket)
    ensure_frontend_bucket_cors(s3, bucket, cors_allowed_origins)

    # Upload app files
    uploaded = upload_frontend_dir(s3, bucket, local_dir)
    log(f"Uploaded frontend files: {uploaded} file(s) from {local_dir}")

    # Upload runtime config (overrides any defaults)
    put_frontend_config_js(s3, bucket, api_base_url, detector_lambda_url)
    log("Uploaded frontend config: s3://%s/config.js" % bucket)

    return f"http://{bucket}.s3-website-{region}.amazonaws.com"


# -------------------------
# NEW: S3 helpers (AUTO bucket creation if not writeable)
# -------------------------
def _s3_can_write(s3, bucket: str) -> bool:
    test_key = f"LifeShot/_bootstrap_write_test_{int(time.time())}.txt"
    try:
        s3.put_object(Bucket=bucket, Key=test_key, Body=b"ok")
        try:
            s3.delete_object(Bucket=bucket, Key=test_key)
        except ClientError:
            pass
        return True
    except ClientError as e:
        code = e.response.get("Error", {}).get("Code", "")
        if code in ("AccessDenied", "AllAccessDisabled", "403"):
            return False
        return False

def s3_prefix_has_files(s3, bucket: str, prefix: str) -> bool:
    """Return True if there is at least one real object under prefix."""
    if not prefix.endswith("/"):
        prefix += "/"
    resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, MaxKeys=5)
    contents = resp.get("Contents", []) or []
    # ignore the folder-marker object itself (if exists)
    for obj in contents:
        key = obj.get("Key", "")
        if key and key != prefix:
            return True
    return False

def _numeric_sort_key(path: str):
    """
    Extract first number from filename for proper ordering:
    1.png, 2.png, 10.png  -> 1,2,10
    Test2_1.png, Test2_12.png -> 1,12
    """
    name = os.path.basename(path)
    m = re.search(r"(\d+)", name)
    return int(m.group(1)) if m else 0


def upload_images_to_prefix(s3, bucket: str, local_dir: str, prefix: str) -> int:
    files = list_image_files(local_dir)
    files.sort(key=_numeric_sort_key)

    if not prefix.endswith("/"):
        prefix = prefix + "/"

    uploaded = 0
    for idx, fp in enumerate(files, start=1):
        key = f"{prefix}{idx:05d}_{os.path.basename(fp)}"

        s3.upload_file(
            fp,
            bucket,
            key,
            ExtraArgs={
                # bonus: keep explicit ordering metadata (optional)
                "Metadata": {
                    "frame_index": str(idx)
                }
            }
        )

        uploaded += 1

    log(f"Uploaded {uploaded} images to s3://{bucket}/{prefix} in numeric order")
    return uploaded

def find_existing_writeable_bucket(s3, base_name: str, account_id: str) -> Optional[str]:
    """
    Finds an existing bucket created by this script, e.g.
    lifeshot-pool-images-045179055320-xxxxxx
    and returns the first one that is writeable.
    """
    prefix = f"{base_name}-{account_id}-".lower()
    try:
        resp = s3.list_buckets()
        buckets = [b["Name"] for b in resp.get("Buckets", [])]
    except ClientError:
        return None

    candidates = [b for b in buckets if b.lower().startswith(prefix)]
    # Prefer deterministic order (oldest->newest); doesn't matter much, we just want "one that works"
    candidates.sort()

    for b in candidates:
        if _s3_can_write(s3, b):
            return b
    return None


def ensure_bucket_writeable(s3, desired_bucket: str, region: str, account_id: str) -> str:
    """
    Return a bucket that is WRITEABLE:
    1) If desired exists and writeable -> use it
    2) If desired is blocked/inaccessible -> try to reuse an existing script-created bucket
    3) If none found -> create ONE new bucket (unique), and reuse it in future runs
    """
    # 1) try desired directly
    try:
        s3.head_bucket(Bucket=desired_bucket)
        log(f"S3 bucket exists: {desired_bucket}")
        if _s3_can_write(s3, desired_bucket):
            log(f"S3 bucket is writeable: {desired_bucket}")
            return desired_bucket
        log(f"S3 bucket exists but NOT writeable. Will try reuse/create another.")
    except ClientError as e:
        code = e.response.get("Error", {}).get("Code", "")
        if code in ("404", "NoSuchBucket", "NotFound"):
            log(f"S3 bucket not found: {desired_bucket} (will try create).")
            try:
                if region == "us-east-1":
                    s3.create_bucket(Bucket=desired_bucket)
                else:
                    s3.create_bucket(
                        Bucket=desired_bucket,
                        CreateBucketConfiguration={"LocationConstraint": region},
                    )
                time.sleep(2)
                if _s3_can_write(s3, desired_bucket):
                    log(f"Created and writeable : {desired_bucket}")
                    return desired_bucket
                log(f"Created {desired_bucket} but still not writeable.")
            except ClientError as e2:
                log(f"Could not create desired bucket {desired_bucket}: {e2}.")
        else:
            # AccessDenied/403 etc.
            log(f"head_bucket failed for {desired_bucket} ({code}).")

    # 2) REUSE: if we already created a bucket in previous runs, reuse it
    existing = find_existing_writeable_bucket(s3, desired_bucket, account_id)
    if existing:
        log(f"Reusing existing writeable bucket : {existing}")
        return existing

    # 3) create a new unique bucket (ONLY if none exists)
    suffix = uuid.uuid4().hex[:6]
    new_bucket = f"{desired_bucket}-{account_id}-{suffix}".lower()

    log(f"Creating new S3 bucket: {new_bucket}")
    if region == "us-east-1":
        s3.create_bucket(Bucket=new_bucket)
    else:
        s3.create_bucket(
            Bucket=new_bucket,
            CreateBucketConfiguration={"LocationConstraint": region},
        )
    time.sleep(2)

    if not _s3_can_write(s3, new_bucket):
        raise RuntimeError(f"Created bucket but cannot write to it: {new_bucket}")

    log(f"Created new writeable bucket: {new_bucket}")
    return new_bucket


def ensure_s3_prefix_object(s3, bucket: str, prefix: str) -> None:
    if not prefix.endswith("/"):
        prefix = prefix + "/"
    try:
        s3.head_object(Bucket=bucket, Key=prefix)
        return
    except ClientError:
        pass
    try:
        s3.put_object(Bucket=bucket, Key=prefix, Body=b"")
    except ClientError:
        pass


def find_first_existing_dir(candidates: List[str]) -> Optional[str]:
    for p in candidates:
        if os.path.isdir(p):
            return p
    return None


def list_image_files(dir_path: str) -> List[str]:
    allowed = {".jpg", ".jpeg", ".png", ".webp", ".bmp", ".gif", ".tiff"}
    out: List[str] = []
    for root, _, files in os.walk(dir_path):
        for f in files:
            ext = os.path.splitext(f)[1].lower()
            if ext in allowed:
                out.append(os.path.join(root, f))
    return out


def upload_images_to_prefix(s3, bucket: str, local_dir: str, prefix: str) -> int:
    files = list_image_files(local_dir)
    if not files:
        log(f"No image files found in: {local_dir}")
        return 0

    if not prefix.endswith("/"):
        prefix = prefix + "/"

    uploaded = 0
    for fp in files:
        key = prefix + os.path.basename(fp)
        s3.upload_file(fp, bucket, key)
        uploaded += 1
    return uploaded


def ensure_s3_structure_and_upload(s3, desired_bucket: str, base_prefix: str, here: str, region: str, account_id: str) -> Dict[str, Any]:
    """
    Ensures bucket is WRITEABLE (auto-create if needed),
    creates prefix markers (best-effort),
    uploads images from deploy/images/Test1 & Test2 into:
      LifeShot/DrowningSet/Test1/
      LifeShot/DrowningSet/Test2/
    """
    bucket = ensure_bucket_writeable(s3, desired_bucket, region, account_id)

    # Ensure prefix markers (best-effort)
    want = [
        "LifeShot/",
        "LifeShot/DrowningSet/",
        "LifeShot/DrowningSet/Test1/",
        "LifeShot/DrowningSet/Test2/",
    ]
    for p in want:
        ensure_s3_prefix_object(s3, bucket, p)

    # Find local deploy folders
    cand_test1 = [
        os.path.join(here, "deploy", "images", "Test1"),
        os.path.join(here, "DEPLOY", "images", "Test1"),
        os.path.join(here, "images", "Test1"),
        os.path.join(here, "deploy", "Test1"),
        os.path.join(here, "DEPLOY", "Test1"),
    ]
    cand_test2 = [
        os.path.join(here, "deploy", "images", "Test2"),
        os.path.join(here, "DEPLOY", "images", "Test2"),
        os.path.join(here, "images", "Test2"),
        os.path.join(here, "deploy", "Test2"),
        os.path.join(here, "DEPLOY", "Test2"),
    ]

    dir_test1 = find_first_existing_dir(cand_test1)
    dir_test2 = find_first_existing_dir(cand_test2)

    uploaded1 = 0
    uploaded2 = 0

    if dir_test1:
        uploaded1 = upload_images_to_prefix_if_needed(s3, bucket, dir_test1, "LifeShot/DrowningSet/Test1/")
        log(f"Uploaded Test1 images: {uploaded1} file(s) from {dir_test1}")
    else:
        log("Test1 local folder not found. Looked in: " + ", ".join(cand_test1))

    if dir_test2:  
        uploaded2 = upload_images_to_prefix_if_needed(s3, bucket, dir_test2, "LifeShot/DrowningSet/Test2/")
        log(f"Uploaded Test2 images: {uploaded2} file(s) from {dir_test2}")
    else:
        log("Test2 local folder not found. Looked in: " + ", ".join(cand_test2))

    return {
        "bucket": bucket,
        "uploaded_test1": uploaded1,
        "uploaded_test2": uploaded2,
        "s3_prefix_test1": "LifeShot/DrowningSet/Test1/",
        "s3_prefix_test2": "LifeShot/DrowningSet/Test2/",
    }


def upload_images_to_prefix_if_needed(s3, bucket: str, local_dir: str, prefix: str) -> int:
    # אם כבר יש קבצים ב־prefix הזה — לא מעלה שוב (כדי לא לשכפל כל ריצה)
    if s3_prefix_has_files(s3, bucket, prefix):
        log(f"Prefix already has files, skipping upload: s3://{bucket}/{prefix}")
        return 0
    return upload_images_to_prefix_ordered(s3, bucket, local_dir, prefix)


def upload_images_to_prefix_ordered(s3, bucket: str, local_dir: str, prefix: str) -> int:
    files = list_image_files(local_dir)
    if not files:
        log(f"No image files found in: {local_dir}")
        return 0

    # ממיין לפי המספר בשם הקובץ (1.jpg, 2.jpg, 10.jpg וכו’)
    files.sort(key=_numeric_sort_key)

    if not prefix.endswith("/"):
        prefix += "/"

    uploaded = 0
    for idx, fp in enumerate(files, start=1):
        ext = os.path.splitext(fp)[1].lower() or ".jpg"
        # KEY מסודר תמיד: 00001.jpg, 00002.jpg ...
        key = f"{prefix}{idx:05d}{ext}"

        s3.upload_file(fp, bucket, key)
        uploaded += 1

    log(f"Uploaded {uploaded} images to s3://{bucket}/{prefix} (ordered, zero-padded)")
    return uploaded

# -------------------------
# DynamoDB helpers
# -------------------------
def ensure_dynamodb_table(ddb, table_name: str) -> None:
    try:
        ddb.describe_table(TableName=table_name)
        log(f"DynamoDB table exists: {table_name}")
        return
    except ClientError as e:
        code = e.response.get("Error", {}).get("Code", "")
        if code != "ResourceNotFoundException":
            raise

    log(f"Creating DynamoDB table: {table_name}")
    ddb.create_table(
        TableName=table_name,
        AttributeDefinitions=[{"AttributeName": "eventId", "AttributeType": "S"}],
        KeySchema=[{"AttributeName": "eventId", "KeyType": "HASH"}],
        BillingMode="PAY_PER_REQUEST",
    )
    waiter = ddb.get_waiter("table_exists")
    waiter.wait(TableName=table_name)
    log(f"Created DynamoDB table: {table_name}")


# -------------------------
# IAM policy for S3 + Dynamo (regular accounts only)
# -------------------------
def ensure_role_data_access_policy(
    iam,
    role_name: str,
    region: str,
    account_id: str,
    bucket: str,
    table_name: str,
) -> None:
    policy_name = "LifeShotDataAccess"
    table_arn = f"arn:aws:dynamodb:{region}:{account_id}:table/{table_name}"
    bucket_arn = f"arn:aws:s3:::{bucket}"
    bucket_objects_arn = f"arn:aws:s3:::{bucket}/*"

    policy_doc = {
        "Version": "2012-10-17",
        "Statement": [
            {"Sid": "LifeShotS3List", "Effect": "Allow", "Action": ["s3:ListBucket"], "Resource": [bucket_arn]},
            {
                "Sid": "LifeShotS3Objects",
                "Effect": "Allow",
                "Action": ["s3:GetObject", "s3:PutObject", "s3:DeleteObject"],
                "Resource": [bucket_objects_arn],
            },
            {
                "Sid": "LifeShotDynamo",
                "Effect": "Allow",
                "Action": [
                    "dynamodb:PutItem",
                    "dynamodb:UpdateItem",
                    "dynamodb:GetItem",
                    "dynamodb:Query",
                    "dynamodb:Scan",
                    "dynamodb:DescribeTable",
                ],
                "Resource": [table_arn],
            },
        ],
    }

    try:
        iam.put_role_policy(
            RoleName=role_name,
            PolicyName=policy_name,
            PolicyDocument=json.dumps(policy_doc),
        )
        log(f"Updated inline IAM policy on role {role_name}: {policy_name}")
    except ClientError as e:
        log(f"Could not attach inline data policy (maybe blocked in lab): {e}")


# -------------------------
# Detect IAM privileges + ensure role in regular accounts
# -------------------------
def has_iam_admin_like_permissions(iam) -> bool:
    try:
        iam.get_account_authorization_details(MaxItems=1)
        return True
    except ClientError:
        pass

    role_name = f"LifeShotIamProbeRole-{int(time.time())}"
    assume_role = {
        "Version": "2012-10-17",
        "Statement": [{
            "Effect": "Allow",
            "Principal": {"Service": "lambda.amazonaws.com"},
            "Action": "sts:AssumeRole"
        }]
    }
    try:
        iam.create_role(
            RoleName=role_name,
            AssumeRolePolicyDocument=json.dumps(assume_role),
            Description="Probe role for LifeShot bootstrap permission detection",
        )
        try:
            iam.delete_role(RoleName=role_name)
        except ClientError:
            pass
        return True
    except ClientError as e:
        code = e.response.get("Error", {}).get("Code", "")
        if code in ("AccessDenied", "AccessDeniedException", "UnauthorizedOperation"):
            return False
        return False


def ensure_lambda_execution_role(iam, account_id: str, role_name: str) -> str:
    assume_role = {
        "Version": "2012-10-17",
        "Statement": [{
            "Effect": "Allow",
            "Principal": {"Service": "lambda.amazonaws.com"},
            "Action": "sts:AssumeRole"
        }]
    }

    try:
        role = iam.get_role(RoleName=role_name)["Role"]
    except ClientError as e:
        if e.response.get("Error", {}).get("Code") != "NoSuchEntity":
            raise
        role = iam.create_role(
            RoleName=role_name,
            AssumeRolePolicyDocument=json.dumps(assume_role),
            Description="LifeShot Lambda execution role (created by bootstrap)",
        )["Role"]
        time.sleep(2)

    iam.attach_role_policy(
        RoleName=role_name,
        PolicyArn="arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole",
    )

    return role["Arn"]


# -------------------------
# Node Login Lambda source
# -------------------------
LOGIN_MJS = r"""import {
  CognitoIdentityProviderClient,
  InitiateAuthCommand,
  RespondToAuthChallengeCommand,
} from "@aws-sdk/client-cognito-identity-provider";

const REGION = process.env.COGNITO_REGION || "us-east-1";
const CLIENT_ID = process.env.COGNITO_CLIENT_ID;
const ALLOWED_ORIGIN = process.env.ALLOWED_ORIGIN || "http://localhost:5500";

if (!CLIENT_ID) console.warn("Missing env COGNITO_CLIENT_ID");

const cognito = new CognitoIdentityProviderClient({ region: REGION });

function corsHeaders(origin) {
  const reqOrigin = origin || "";
  const allowOrigin = reqOrigin && reqOrigin === ALLOWED_ORIGIN ? reqOrigin : ALLOWED_ORIGIN;

  return {
    "Access-Control-Allow-Origin": allowOrigin,
    "Access-Control-Allow-Headers": "content-type,authorization",
    "Access-Control-Allow-Methods": "GET,POST,PATCH,OPTIONS",
    "Access-Control-Allow-Credentials": "false",
    "Vary": "Origin",
  };
}

function json(statusCode, bodyObj, origin, extraHeaders = {}) {
  return {
    statusCode,
    headers: {
      "Content-Type": "application/json",
      ...corsHeaders(origin),
      ...extraHeaders,
    },
    body: JSON.stringify(bodyObj),
  };
}

function parseJsonBody(event) {
  try {
    if (!event.body) return {};
    return typeof event.body === "string" ? JSON.parse(event.body) : event.body;
  } catch {
    return {};
  }
}

function decodeJwtPayload(token) {
  try {
    const parts = String(token || "").split(".");
    if (parts.length < 2) return null;
    const b64 = parts[1].replace(/-/g, "+").replace(/_/g, "/");
    const padded = b64 + "===".slice((b64.length + 3) % 4);
    const jsonStr = Buffer.from(padded, "base64").toString("utf8");
    return JSON.parse(jsonStr);
  } catch {
    return null;
  }
}

function getRoleFromGroups(groups) {
  const g = (groups || []).map((x) => String(x).toLowerCase());
  if (g.includes("admins") || g.includes("admin")) return "admin";
  if (g.includes("lifeguards") || g.includes("lifeguard") || g.includes("guard")) return "guard";
  return "unknown";
}

function getBearerToken(event) {
  const h = event.headers || {};
  const auth = h.authorization || h.Authorization || "";
  const m = String(auth).match(/^Bearer\s+(.+)$/i);
  return m ? m[1].trim() : "";
}

async function routeLogin(event, origin) {
  const body = parseJsonBody(event);
  const username = String(body.username || "").trim();
  const password = String(body.password || "").trim();

  if (!username || !password) {
    return json(400, { message: "username and password are required" }, origin);
  }

  const cmd = new InitiateAuthCommand({
    AuthFlow: "USER_PASSWORD_AUTH",
    ClientId: CLIENT_ID,
    AuthParameters: { USERNAME: username, PASSWORD: password },
  });

  try {
    const resp = await cognito.send(cmd);

    if (resp.ChallengeName === "NEW_PASSWORD_REQUIRED") {
      const challengeUsername =
        resp.ChallengeParameters?.USER_ID_FOR_SRP || username;

      return json(
        409,
        {
          ok: false,
          challenge: "NEW_PASSWORD_REQUIRED",
          session: resp.Session || "",
          username: challengeUsername,
          message: "New password required",
        },
        origin
      );
    }

    const auth = resp.AuthenticationResult || {};
    const accessToken = auth.AccessToken || "";
    const idToken = auth.IdToken || "";
    const refreshToken = auth.RefreshToken || "";
    const expiresIn = Number(auth.ExpiresIn || 3600);

    if (!accessToken || !idToken) {
      return json(401, { message: "Login failed (no tokens returned)" }, origin);
    }

    const payload = decodeJwtPayload(idToken) || {};
    const groups = Array.isArray(payload["cognito:groups"]) ? payload["cognito:groups"] : [];
    const role = getRoleFromGroups(groups);

    return json(
      200,
      {
        ok: true,
        role,
        username: payload["cognito:username"] || username,
        email: payload["email"] || "",
        groups,
        expiresIn,
        accessToken,
        idToken,
        refreshToken,
      },
      origin
    );
  } catch (err) {
    return json(
      401,
      { message: "Login failed", detail: err?.name || "AuthError", reason: err?.message || "" },
      origin
    );
  }
}

async function routeCompletePassword(event, origin) {
  const body = parseJsonBody(event);
  const username = String(body.username || "").trim();
  const newPassword = String(body.newPassword || "").trim();
  const session = String(body.session || "").trim();

  if (!username || !newPassword || !session) {
    return json(400, { message: "username, newPassword and session are required" }, origin);
  }

  const cmd = new RespondToAuthChallengeCommand({
    ClientId: CLIENT_ID,
    ChallengeName: "NEW_PASSWORD_REQUIRED",
    Session: session,
    ChallengeResponses: {
      USERNAME: username,
      NEW_PASSWORD: newPassword,
    },
  });

  try {
    const resp = await cognito.send(cmd);
    const auth = resp.AuthenticationResult || {};

    const accessToken = auth.AccessToken || "";
    const idToken = auth.IdToken || "";
    const refreshToken = auth.RefreshToken || "";
    const expiresIn = Number(auth.ExpiresIn || 3600);

    if (!accessToken || !idToken) {
      return json(401, { message: "Password change succeeded but no tokens returned" }, origin);
    }

    const payload = decodeJwtPayload(idToken) || {};
    const groups = Array.isArray(payload["cognito:groups"]) ? payload["cognito:groups"] : [];
    const role = getRoleFromGroups(groups);

    return json(
      200,
      {
        ok: true,
        role,
        username: payload["cognito:username"] || username,
        email: payload["email"] || "",
        groups,
        expiresIn,
        accessToken,
        idToken,
        refreshToken,
      },
      origin
    );
  } catch (err) {
    return json(
      401,
      { message: "Password change failed", detail: err?.name || "ChallengeError", reason: err?.message || "" },
      origin
    );
  }
}

async function routeMe(event, origin) {
  const token = getBearerToken(event);
  if (!token) return json(401, { ok: false, message: "Missing Bearer token" }, origin);

  const payload = decodeJwtPayload(token);
  if (!payload) return json(401, { ok: false, message: "Invalid token" }, origin);

  const groups = Array.isArray(payload["cognito:groups"]) ? payload["cognito:groups"] : [];
  const role = getRoleFromGroups(groups);

  return json(
    200,
    {
      ok: true,
      role,
      username: payload["cognito:username"] || payload["username"] || "",
      email: payload["email"] || "",
      groups,
    },
    origin
  );
}

async function routeLogout(event, origin) {
  return json(200, { ok: true }, origin);
}

export const handler = async (event) => {
  const method = event.requestContext?.http?.method || event.httpMethod || "";
  const path = event.requestContext?.http?.path || event.rawPath || event.path || "";
  const origin = event.headers?.origin || event.headers?.Origin || "";

  if (method === "OPTIONS") {
    return { statusCode: 204, headers: corsHeaders(origin), body: "" };
  }

  if (method === "POST" && path.endsWith("/auth/login")) return routeLogin(event, origin);
  if (method === "POST" && path.endsWith("/auth/complete-password")) return routeCompletePassword(event, origin);
  if (method === "GET"  && path.endsWith("/auth/me")) return routeMe(event, origin);
  if (method === "POST" && path.endsWith("/auth/logout")) return routeLogout(event, origin);

  return json(404, { message: "Not found", path, method }, origin);
};
"""

PACKAGE_JSON = {
    "name": "lifeshot-login",
    "version": "1.0.0",
    "type": "module",
    "dependencies": {
        "@aws-sdk/client-cognito-identity-provider": "^3.600.0"
    }
}


# -------------------------
# Placeholder Lambda code to allow full "from 0"
# -------------------------
def make_placeholder_zip(module_name: str, out_zip: str) -> None:
    with tempfile.TemporaryDirectory() as td:
        py_path = os.path.join(td, f"{module_name}.py")

        if module_name == "events_and_sns":
            with open(py_path, "w", encoding="utf-8") as f:
                f.write(
                    "import os, json, time\n"
                    "import boto3\n"
                    "TABLE = os.getenv('EVENTS_TABLE_NAME', '')\n"
                    "ddb = boto3.client('dynamodb')\n"
                    "def _now():\n"
                    "    return time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())\n"
                    "def lambda_handler(event, context):\n"
                    "    body = event or {}\n"
                    "    if isinstance(body, dict) and 'body' in body and isinstance(body['body'], str):\n"
                    "        try:\n"
                    "            body = json.loads(body['body'])\n"
                    "        except Exception:\n"
                    "            body = event or {}\n"
                    "    event_id = str(body.get('eventId') or body.get('id') or f'EVT-{int(time.time())}')\n"
                    "    item = {'eventId': {'S': event_id}, 'created_at': {'S': str(body.get('created_at') or _now())}}\n"
                    "    if body.get('closedAt') is not None:\n"
                    "        item['closedAt'] = {'S': str(body.get('closedAt'))}\n"
                    "    if body.get('prevImageKey') is not None:\n"
                    "        item['prevImageKey'] = {'S': str(body.get('prevImageKey'))}\n"
                    "    if body.get('responseSeconds') is not None:\n"
                    "        try:\n"
                    "            item['responseSeconds'] = {'N': str(float(body.get('responseSeconds')))}\n"
                    "        except Exception:\n"
                    "            pass\n"
                    "    wrote = False\n"
                    "    if TABLE:\n"
                    "        try:\n"
                    "            ddb.put_item(TableName=TABLE, Item=item)\n"
                    "            wrote = True\n"
                    "        except Exception:\n"
                    "            wrote = False\n"
                    "    return {'statusCode': 200, 'headers': {'Content-Type': 'application/json'}, 'body': json.dumps({'ok': True, 'module': __name__, 'wroteToDdb': wrote, 'eventId': event_id, 'table': TABLE})}\n"
                )
        else:
            with open(py_path, "w", encoding="utf-8") as f:
                f.write(
                    "import json\n"
                    "def lambda_handler(event, context):\n"
                    "    return {'statusCode': 200, 'headers': {'Content-Type': 'application/json'}, 'body': json.dumps({'ok': True, 'module': __name__, 'note': 'PLACEHOLDER - replace with real code'})}\n"
                )

        zip_from_dir(td, out_zip)


def make_events_api_placeholder_zip(out_zip: str) -> None:
    with tempfile.TemporaryDirectory() as td:
        py_path = os.path.join(td, "lambda_function.py")
        with open(py_path, "w", encoding="utf-8") as f:
            f.write(
                "import os, json, time\n"
                "import boto3\n"
                "TABLE = os.getenv('EVENTS_TABLE_NAME', '')\n"
                "ddb = boto3.client('dynamodb')\n"
                "def _now():\n"
                "    return time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())\n"
                "def lambda_handler(event, context):\n"
                "    method = (event.get('requestContext', {}).get('http', {}).get('method') if isinstance(event, dict) else None) or (event.get('httpMethod') if isinstance(event, dict) else '')\n"
                "    body = {}\n"
                "    try:\n"
                "        raw = event.get('body') if isinstance(event, dict) else None\n"
                "        if isinstance(raw, str) and raw:\n"
                "            body = json.loads(raw)\n"
                "        elif isinstance(raw, dict):\n"
                "            body = raw\n"
                "    except Exception:\n"
                "        body = {}\n"
                "    wrote = False\n"
                "    event_id = str(body.get('eventId') or body.get('id') or f'EVT-{int(time.time())}')\n"
                "    if TABLE and str(method).upper() == 'PATCH':\n"
                "        item = {'eventId': {'S': event_id}, 'created_at': {'S': str(body.get('created_at') or _now())}}\n"
                "        if body.get('closedAt') is not None:\n"
                "            item['closedAt'] = {'S': str(body.get('closedAt'))}\n"
                "        if body.get('prevImageKey') is not None:\n"
                "            item['prevImageKey'] = {'S': str(body.get('prevImageKey'))}\n"
                "        if body.get('responseSeconds') is not None:\n"
                "            try:\n"
                "                item['responseSeconds'] = {'N': str(float(body.get('responseSeconds')))}\n"
                "            except Exception:\n"
                "                pass\n"
                "        try:\n"
                "            ddb.put_item(TableName=TABLE, Item=item)\n"
                "            wrote = True\n"
                "        except Exception:\n"
                "            wrote = False\n"
                "    return {'statusCode': 200, 'headers': {'Content-Type': 'application/json'}, 'body': json.dumps({'ok': True, 'note': 'PLACEHOLDER /events API', 'wroteToDdb': wrote, 'table': TABLE, 'eventId': event_id})}\n"
            )
        zip_from_dir(td, out_zip)


# -------------------------
# Cognito
# -------------------------
def ensure_user_pool(cognito, user_pool_name: str) -> str:
    pools = cognito.list_user_pools(MaxResults=60).get("UserPools", [])
    for p in pools:
        if p.get("Name") == user_pool_name:
            return p["Id"]

    resp = cognito.create_user_pool(
        PoolName=user_pool_name,
        UsernameAttributes=["email"],
        AutoVerifiedAttributes=["email"],
        Policies={
            "PasswordPolicy": {
                "MinimumLength": 8,
                "RequireUppercase": True,
                "RequireLowercase": True,
                "RequireNumbers": True,
                "RequireSymbols": True,
                "TemporaryPasswordValidityDays": 7,
            }
        },
    )
    return resp["UserPool"]["Id"]


def ensure_app_client(cognito, user_pool_id: str, app_client_name: str) -> str:
    clients = cognito.list_user_pool_clients(UserPoolId=user_pool_id, MaxResults=60).get("UserPoolClients", [])
    for c in clients:
        if c.get("ClientName") == app_client_name:
            return c["ClientId"]

    resp = cognito.create_user_pool_client(
        UserPoolId=user_pool_id,
        ClientName=app_client_name,
        GenerateSecret=False,
        ExplicitAuthFlows=["ALLOW_USER_PASSWORD_AUTH", "ALLOW_REFRESH_TOKEN_AUTH"],
        PreventUserExistenceErrors="ENABLED",
    )
    return resp["UserPoolClient"]["ClientId"]


def ensure_group(cognito, user_pool_id: str, group_name: str) -> None:
    try:
        cognito.get_group(UserPoolId=user_pool_id, GroupName=group_name)
        log(f"Group exists: {group_name}")
    except ClientError as e:
        if e.response.get("Error", {}).get("Code") == "ResourceNotFoundException":
            cognito.create_group(UserPoolId=user_pool_id, GroupName=group_name)
            log(f"Created group: {group_name}")
        else:
            raise


def ensure_user(cognito, user_pool_id: str, email: str, temp_password: str) -> None:
    try:
        cognito.admin_get_user(UserPoolId=user_pool_id, Username=email)
        log(f"User exists: {email}")
    except ClientError as e:
        if e.response.get("Error", {}).get("Code") == "UserNotFoundException":
            cognito.admin_create_user(
                UserPoolId=user_pool_id,
                Username=email,
                TemporaryPassword=temp_password,
                MessageAction="SUPPRESS",
                UserAttributes=[
                    {"Name": "email", "Value": email},
                    {"Name": "email_verified", "Value": "true"},
                ],
            )
            log(f"Created user (temp password): {email}")
        else:
            raise


def add_user_to_group(cognito, user_pool_id: str, email: str, group_name: str) -> None:
    cognito.admin_add_user_to_group(UserPoolId=user_pool_id, Username=email, GroupName=group_name)
    log(f"Assigned {email} -> {group_name}")


def set_user_password_confirmed(cognito, user_pool_id: str, email: str, password: str) -> None:
    cognito.admin_set_user_password(
        UserPoolId=user_pool_id,
        Username=email,
        Password=password,
        Permanent=True,
    )
    log(f"Set permanent password for: {email}")


# -------------------------
# Lambda
# -------------------------
def get_lambda_arn(client_lambda, fn_name: str) -> Optional[str]:
    try:
        resp = client_lambda.get_function(FunctionName=fn_name)
        return resp["Configuration"]["FunctionArn"]
    except ClientError as e:
        if e.response.get("Error", {}).get("Code") == "ResourceNotFoundException":
            return None
        raise


def ensure_lambda_function(
    client_lambda,
    fn_name: str,
    role_arn: str,
    runtime: str,
    handler: str,
    zip_bytes: bytes,
    timeout: int,
    memory: int,
    env_vars: Optional[Dict[str, str]] = None,
) -> str:
    arn = get_lambda_arn(client_lambda, fn_name)

    if arn is None:
        resp = retry_on_conflict(lambda: client_lambda.create_function(
            FunctionName=fn_name,
            Runtime=runtime,
            Role=role_arn,
            Handler=handler,
            Timeout=timeout,
            MemorySize=memory,
            Code={"ZipFile": zip_bytes},
            Environment={"Variables": env_vars or {}},
            Publish=True,
        ))
        arn = resp["FunctionArn"]
        log(f"Created Lambda: {fn_name} -> {arn}")
        wait_lambda_active(client_lambda, fn_name)
        return arn

    retry_on_conflict(lambda: client_lambda.update_function_code(
        FunctionName=fn_name,
        ZipFile=zip_bytes,
        Publish=True,
    ))
    wait_lambda_active(client_lambda, fn_name)

    retry_on_conflict(lambda: client_lambda.update_function_configuration(
        FunctionName=fn_name,
        Runtime=runtime,
        Role=role_arn,
        Handler=handler,
        Timeout=timeout,
        MemorySize=memory,
        Environment={"Variables": env_vars or {}},
    ))
    wait_lambda_active(client_lambda, fn_name)

    log(f"Updated Lambda: {fn_name} -> {arn}")
    return arn


def merge_lambda_env_vars(client_lambda, fn_name: str, updates: Dict[str, Optional[str]]) -> None:
    wait_lambda_active(client_lambda, fn_name)
    resp = client_lambda.get_function_configuration(FunctionName=fn_name)
    cur = resp.get("Environment", {}).get("Variables", {}) or {}
    merged = merge_env(cur, updates)

    retry_on_conflict(lambda: client_lambda.update_function_configuration(
        FunctionName=fn_name,
        Environment={"Variables": merged},
    ))
    wait_lambda_active(client_lambda, fn_name)


def ensure_function_url(client_lambda, fn_name: str, allowed_origin: str) -> str:
    wait_lambda_active(client_lambda, fn_name)

    #  Lambda Function URL CORS does NOT accept "OPTIONS" here
    cors_cfg = {
        "AllowOrigins": ["*"],
        "AllowMethods": ["POST"],  # <-- remove OPTIONS
        "AllowHeaders": ["content-type", "authorization"],
        "ExposeHeaders": [],
        "MaxAge": 0,
        "AllowCredentials": False,
    }

    try:
        resp = client_lambda.get_function_url_config(FunctionName=fn_name)
        url = resp["FunctionUrl"]
        retry_on_conflict(lambda: client_lambda.update_function_url_config(
            FunctionName=fn_name,
            AuthType="NONE",
            Cors=cors_cfg,
        ))
        wait_lambda_active(client_lambda, fn_name)
        log("Function URL exists, updated CORS/Auth.")
        return url
    except ClientError as e:
        if e.response.get("Error", {}).get("Code") != "ResourceNotFoundException":
            raise

    resp = retry_on_conflict(lambda: client_lambda.create_function_url_config(
        FunctionName=fn_name,
        AuthType="NONE",
        Cors=cors_cfg,
    ))
    wait_lambda_active(client_lambda, fn_name)
    url = resp["FunctionUrl"]
    log("Created Function URL for Detector (Auth NONE).")
    return url


# -------------------------
# SNS
# -------------------------
def ensure_sns_topic_and_policy(sns, account_id: str, topic_name: str) -> str:
    topic_arn = sns.create_topic(Name=topic_name)["TopicArn"]

    policy = {
        "Version": "2008-10-17",
        "Id": "__default_policy_ID",
        "Statement": [
            {
                "Sid": "__default_statement_ID",
                "Effect": "Allow",
                "Principal": {"AWS": "*"},
                "Action": [
                    "SNS:Publish",
                    "SNS:RemovePermission",
                    "SNS:SetTopicAttributes",
                    "SNS:DeleteTopic",
                    "SNS:ListSubscriptionsByTopic",
                    "SNS:GetTopicAttributes",
                    "SNS:AddPermission",
                    "SNS:Subscribe",
                ],
                "Resource": topic_arn,
                "Condition": {"StringEquals": {"AWS:SourceAccount": account_id}},
            },
            {
                "Sid": "AllowLambdaPublish",
                "Effect": "Allow",
                "Principal": {"Service": "lambda.amazonaws.com"},
                "Action": "SNS:Publish",
                "Resource": topic_arn,
                "Condition": {"StringEquals": {"AWS:SourceAccount": account_id}},
            },
        ],
    }

    sns.set_topic_attributes(
        TopicArn=topic_arn,
        AttributeName="Policy",
        AttributeValue=json.dumps(policy),
    )
    log("SNS Access Policy updated.")
    return topic_arn


def ensure_email_subscription(sns, topic_arn: str, email: str) -> None:
    paginator = sns.get_paginator("list_subscriptions_by_topic")
    for page in paginator.paginate(TopicArn=topic_arn):
        subs = page.get("Subscriptions", [])
        for s in subs:
            if s.get("Endpoint") == email:
                log(f"SNS subscription already exists (or pending): {s.get('SubscriptionArn')}")
                return

    sns.subscribe(TopicArn=topic_arn, Protocol="email", Endpoint=email)
    log(f"SNS subscription created for {email} (check email to CONFIRM).")


# -------------------------
# API Gateway v2 HTTP
# -------------------------
def ensure_http_api(apigw, api_name: str) -> str:
    apis = apigw.get_apis().get("Items", [])
    for a in apis:
        if a.get("Name") == api_name and a.get("ProtocolType") == "HTTP":
            api_id = a["ApiId"]
            apigw.update_api(
                ApiId=api_id,
                CorsConfiguration={
                    "AllowOrigins": ["*"],
                    "AllowHeaders": ["authorization", "content-type"],
                    "AllowMethods": ["GET", "POST", "PATCH", "OPTIONS"],
                    "AllowCredentials": False,
                    "MaxAge": 0,
                },
            )
            log("API exists, updated CORS config.")
            return api_id

    resp = apigw.create_api(
        Name=api_name,
        ProtocolType="HTTP",
        CorsConfiguration={
            "AllowOrigins": ["*"],
            "AllowHeaders": ["authorization", "content-type"],
            "AllowMethods": ["GET", "POST", "PATCH", "OPTIONS"],
            "AllowCredentials": False,
            "MaxAge": 0,
        },
    )
    api_id = resp["ApiId"]
    log(f"Created API: {api_id}")
    return api_id


def ensure_default_stage(apigw, api_id: str) -> None:
    stages = apigw.get_stages(ApiId=api_id).get("Items", [])
    for s in stages:
        if s.get("StageName") == "$default":
            apigw.update_stage(ApiId=api_id, StageName="$default", AutoDeploy=True)
            return
    apigw.create_stage(ApiId=api_id, StageName="$default", AutoDeploy=True)
    log("Created $default stage.")



def ensure_integration(apigw, api_id: str, lambda_arn: str) -> str:
    integrations = apigw.get_integrations(ApiId=api_id).get("Items", [])
    for it in integrations:
        if it.get("IntegrationUri") == lambda_arn:
            return it["IntegrationId"]

    resp = apigw.create_integration(
        ApiId=api_id,
        IntegrationType="AWS_PROXY",
        IntegrationUri=lambda_arn,
        PayloadFormatVersion="2.0",
    )
    return resp["IntegrationId"]


def ensure_jwt_authorizer(apigw, api_id: str, client_id: str, issuer: str) -> str:
    AUTH_NAME = "JWT-Auth"

    auths = apigw.get_authorizers(ApiId=api_id).get("Items", [])
    for a in auths:
        if a.get("AuthorizerType") == "JWT" and a.get("Name") == AUTH_NAME:
            auth_id = a["AuthorizerId"]
            apigw.update_authorizer(
                ApiId=api_id,
                AuthorizerId=auth_id,
                JwtConfiguration={"Audience": [client_id], "Issuer": issuer},
            )
            log("Authorizer exists, updated config.")
            return auth_id

    resp = apigw.create_authorizer(
        ApiId=api_id,
        Name=AUTH_NAME,
        AuthorizerType="JWT",
        IdentitySource=["$request.header.Authorization"],
        JwtConfiguration={"Audience": [client_id], "Issuer": issuer},
    )
    auth_id = resp["AuthorizerId"]
    log(f"Created authorizer: {auth_id}")
    return auth_id


def ensure_route(apigw, api_id: str, route_key: str, integration_id: str, authorization_type: str = "NONE", authorizer_id: Optional[str] = None) -> None:
    routes = apigw.get_routes(ApiId=api_id).get("Items", [])
    existing = None
    for r in routes:
        if r.get("RouteKey") == route_key:
            existing = r
            break

    target = f"integrations/{integration_id}"
    if existing is None:
        kwargs: Dict[str, Any] = {
            "ApiId": api_id,
            "RouteKey": route_key,
            "Target": target,
            "AuthorizationType": authorization_type,
        }
        if authorization_type == "JWT" and authorizer_id:
            kwargs["AuthorizerId"] = authorizer_id
        apigw.create_route(**kwargs)
        log(f"Created route: {route_key} (auth={authorization_type})")
    else:
        kwargs2: Dict[str, Any] = {
            "ApiId": api_id,
            "RouteId": existing["RouteId"],
            "Target": target,
            "AuthorizationType": authorization_type,
        }
        if authorization_type == "JWT" and authorizer_id:
            kwargs2["AuthorizerId"] = authorizer_id
        apigw.update_route(**kwargs2)
        log(f"Updated route: {route_key} (auth={authorization_type})")


def deploy_api(apigw, api_id: str) -> None:
    apigw.create_deployment(ApiId=api_id)
    log("Deployed API stage.")
    time.sleep(2)


# -------------------------
# Build Login Lambda zip
# -------------------------
def build_login_lambda_zip() -> bytes:
    here = os.path.dirname(os.path.abspath(__file__))
    prebuilt = os.path.join(here, "login.zip")

    if os.path.exists(prebuilt):
        log(f"Using prebuilt login.zip: {prebuilt}")
        return b64_zip_bytes(prebuilt)

    need_cmd("node")
    need_cmd("npm")

    with tempfile.TemporaryDirectory() as wd:
        login_path = os.path.join(wd, "login.mjs")
        pkg_path = os.path.join(wd, "package.json")

        with open(login_path, "w", encoding="utf-8") as f:
            f.write(LOGIN_MJS)

        with open(pkg_path, "w", encoding="utf-8") as f:
            json.dump(PACKAGE_JSON, f, indent=2)

        log("Packaging Login Lambda zip (npm install)...")
        subprocess.run(["npm", "install"], cwd=wd, check=True)

        out_zip = os.path.join(wd, "lifeshot_login.zip")
        with zipfile.ZipFile(out_zip, "w", compression=zipfile.ZIP_DEFLATED) as z:
            z.write(login_path, "login.mjs")
            z.write(pkg_path, "package.json")
            nm_dir = os.path.join(wd, "node_modules")
            for root, _, files in os.walk(nm_dir):
                for f2 in files:
                    full = os.path.join(root, f2)
                    rel = os.path.relpath(full, wd)
                    z.write(full, rel)

        return b64_zip_bytes(out_zip)

# -------------------------
# Main
# -------------------------
def main() -> None:
    global FRAMES_BUCKET  # so we can auto-switch bucket if needed

    log(f"Region: {REGION}")

    sts = safe_client("sts")
    iam = safe_client("iam")
    cognito = safe_client("cognito-idp")
    client_lambda = safe_client("lambda")
    apigw = safe_client("apigatewayv2")
    sns = safe_client("sns")
    s3 = safe_client("s3")
    ddb = safe_client("dynamodb")

    # credentials sanity
    try:
        sts.get_caller_identity()
    except ClientError as e:
        die(f"AWS credentials not working: {e}")

    account_id = get_account_id(sts)
    log(f"Account: {account_id}")

    # ------------------------------------------------------------
    #  IAM / Role selection (robust)
    #   - Try to use/create LifeShotLambdaRole if possible
    #   - If CreateRole is blocked (VocLabs), fallback to LabRole
    # ------------------------------------------------------------
    LAMBDA_ROLE_NAME_REGULAR = os.getenv("LAMBDA_ROLE_NAME_REGULAR", "LifeShotLambdaRole")

    role_name_in_use = LAB_ROLE_NAME
    role_arn: Optional[str] = None

    # Detect ability (but do NOT trust it blindly; we still catch CreateRole failure)
    can_manage_iam = has_iam_admin_like_permissions(iam)

    if can_manage_iam:
        log("Detected IAM privileges (regular account/admin). Trying to create/use LifeShotLambdaRole...")
        try:
            role_arn = ensure_lambda_execution_role(iam, account_id, LAMBDA_ROLE_NAME_REGULAR)
            role_name_in_use = LAMBDA_ROLE_NAME_REGULAR
            log(f"Using Lambda role: {role_arn}")
        except ClientError as e:
            code = e.response.get("Error", {}).get("Code", "")
            msg = e.response.get("Error", {}).get("Message", "") or str(e)

            #  fallback instead of failing
            if code in ("AccessDenied", "AccessDeniedException", "UnauthorizedOperation") or "CreateRole" in msg:
                log("IAM CreateRole is blocked in this environment. Falling back to LabRole (academy style).")
                can_manage_iam = False
            else:
                raise

    if not role_arn:
        log("Using LabRole (academy style).")
        role_name_in_use = LAB_ROLE_NAME
        try:
            role_arn = iam.get_role(RoleName=LAB_ROLE_NAME)["Role"]["Arn"]
        except ClientError:
            role_arn = f"arn:aws:iam::{account_id}:role/{LAB_ROLE_NAME}"
        log(f"Using LAB ROLE for Lambdas: {role_arn}")

    here = os.path.dirname(os.path.abspath(__file__))

    # -------------------------
    #  S3 auto bucket + upload
    # -------------------------
    log("Ensuring S3 bucket + folder structure + uploading images from DEPLOY...")
    s3_result = ensure_s3_structure_and_upload(
        s3=s3,
        desired_bucket=FRAMES_BUCKET,
        base_prefix=FRAMES_PREFIX,
        here=here,
        region=REGION,
        account_id=account_id,
    )

    # If we had to create a new bucket, switch FRAMES_BUCKET globally (used later for env vars + summary)
    if s3_result.get("bucket") and s3_result["bucket"] != FRAMES_BUCKET:
        log(f"Switching FRAMES_BUCKET -> {s3_result['bucket']}")
        FRAMES_BUCKET = s3_result["bucket"]

    # -------------------------
    # DynamoDB table
    # -------------------------
    log("Ensuring DynamoDB events table...")
    ensure_dynamodb_table(ddb, EVENTS_TABLE_NAME)

    # If regular account role, attach data access policy (S3 + Dynamo)
    if can_manage_iam:
        ensure_role_data_access_policy(
            iam=iam,
            role_name=role_name_in_use,
            region=REGION,
            account_id=account_id,
            bucket=FRAMES_BUCKET,
            table_name=EVENTS_TABLE_NAME,
        )

    # -------------------------
    # Cognito
    # -------------------------
    log(f"Ensuring Cognito User Pool: {USER_POOL_NAME}")
    user_pool_id = ensure_user_pool(cognito, USER_POOL_NAME)
    user_pool_arn = f"arn:aws:cognito-idp:{REGION}:{account_id}:userpool/{user_pool_id}"
    issuer = f"https://cognito-idp.{REGION}.amazonaws.com/{user_pool_id}"
    log(f"User Pool ID: {user_pool_id}")
    log(f"UserPool ARN: {user_pool_arn}")
    log(f"Issuer: {issuer}")

    log(f"Ensuring App Client: {APP_CLIENT_NAME}")
    client_id = ensure_app_client(cognito, user_pool_id, APP_CLIENT_NAME)
    log(f"Client ID: {client_id}")

    log(f"Ensuring groups: {GROUP_ADMINS}, {GROUP_LIFEGUARDS}")
    ensure_group(cognito, user_pool_id, GROUP_ADMINS)
    ensure_group(cognito, user_pool_id, GROUP_LIFEGUARDS)

    log("Ensuring users exist + group assignments")
    ensure_user(cognito, user_pool_id, ADMIN_EMAIL, TEMP_PASSWORD)
    ensure_user(cognito, user_pool_id, GUARD_EMAIL, TEMP_PASSWORD)
    set_user_password_confirmed(cognito, user_pool_id, ADMIN_EMAIL, TEMP_PASSWORD)
    set_user_password_confirmed(cognito, user_pool_id, GUARD_EMAIL, TEMP_PASSWORD)

    add_user_to_group(cognito, user_pool_id, ADMIN_EMAIL, GROUP_ADMINS)
    add_user_to_group(cognito, user_pool_id, GUARD_EMAIL, GROUP_LIFEGUARDS)

    # -------------------------
    # Login Lambda
    # -------------------------
    login_zip = build_login_lambda_zip()
    login_env = {
        "COGNITO_REGION": REGION,
        "COGNITO_CLIENT_ID": client_id,
        "ALLOWED_ORIGIN": ALLOWED_ORIGIN,
    }
    login_arn = ensure_lambda_function(
        client_lambda=client_lambda,
        fn_name=LOGIN_LAMBDA_NAME,
        role_arn=role_arn,
        runtime=LOGIN_RUNTIME,
        handler="login.handler",
        zip_bytes=login_zip,
        timeout=15,
        memory=256,
        env_vars=login_env,
    )

    # -------------------------
    # Split Lambdas
    # -------------------------
    log("Ensuring split Lambdas exist (Detector / Render / Events+SNS). Prefer deploy zips, fallback to placeholders if missing...")

    det_real = os.path.join(here, "detector_logic.zip")
    ren_real = os.path.join(here, "render_and_s3.zip")
    es_real = os.path.join(here, "events_and_sns.zip")

    def load_or_placeholder(real_path: str, module_name: str) -> bytes:
        if os.path.exists(real_path):
            log(f"Using zip: {real_path}")
            return b64_zip_bytes(real_path)
        log(f"Missing {real_path} -> creating placeholder zip for {module_name}")
        with tempfile.TemporaryDirectory() as td:
            outp = os.path.join(td, f"{module_name}.zip")
            make_placeholder_zip(module_name, outp)
            return b64_zip_bytes(outp)

    det_zip_bytes = load_or_placeholder(det_real, "detector_logic")
    ren_zip_bytes = load_or_placeholder(ren_real, "render_and_s3")
    es_zip_bytes = load_or_placeholder(es_real, "events_and_sns")

    detector_arn = ensure_lambda_function(
        client_lambda, DETECTOR_LAMBDA_NAME, role_arn, DEFAULT_PY_RUNTIME, DETECTOR_HANDLER,
        det_zip_bytes, DETECTOR_TIMEOUT, DETECTOR_MEMORY, env_vars={}
    )
    render_arn = ensure_lambda_function(
        client_lambda, RENDER_LAMBDA_NAME, role_arn, DEFAULT_PY_RUNTIME, RENDER_HANDLER,
        ren_zip_bytes, RENDER_TIMEOUT, RENDER_MEMORY, env_vars={}
    )
    events_sns_arn = ensure_lambda_function(
        client_lambda, EVENTS_SNS_LAMBDA_NAME, role_arn, DEFAULT_PY_RUNTIME, EVENTS_SNS_HANDLER,
        es_zip_bytes, EVENTS_SNS_TIMEOUT, EVENTS_SNS_MEMORY, env_vars={}
    )

    # ============================================================
    # Publish + Attach Pillow Layer (from deploy/pillow311.zip)
    # ============================================================
    log("Ensuring Pillow (PIL) Layer is published + attached to Detector + Render...")

    pillow_zip_path = os.path.join(here, "pillow311.zip")
    if not os.path.exists(pillow_zip_path):
        die(f"Missing Pillow layer zip: {pillow_zip_path}")

    layer_resp = retry_on_conflict(lambda: client_lambda.publish_layer_version(
        LayerName="LifeShot-Pillow",
        Content={"ZipFile": b64_zip_bytes(pillow_zip_path)},
        CompatibleRuntimes=[DEFAULT_PY_RUNTIME],
        Description=f"LifeShot Pillow layer ({now_utc_iso()})",
    ))
    pillow_layer_version_arn = layer_resp["LayerVersionArn"]
    log(f"Published Pillow layer version: {pillow_layer_version_arn}")

    def _attach_layer(fn_name: str):
        wait_lambda_active(client_lambda, fn_name)
        cfg = client_lambda.get_function_configuration(FunctionName=fn_name)
        existing = cfg.get("Layers", []) or []

        kept = []
        for x in existing:
            arn = (x.get("Arn") or x.get("LayerVersionArn") or "").strip()
            # remove older versions of our layer
            if ":layer:LifeShot-Pillow:" in arn:
                continue
            if arn:
                kept.append(arn)

        if pillow_layer_version_arn not in kept:
            kept.append(pillow_layer_version_arn)

        retry_on_conflict(lambda: client_lambda.update_function_configuration(
            FunctionName=fn_name,
            Layers=kept,
        ))
        wait_lambda_active(client_lambda, fn_name)
        log(f"Attached/Updated Pillow layer on {fn_name}: {pillow_layer_version_arn}")

    _attach_layer(DETECTOR_LAMBDA_NAME)
    _attach_layer(RENDER_LAMBDA_NAME)


    # -------------------------
    # SNS
    # -------------------------
    log("Ensuring SNS Topic + Access Policy + Email subscription + set SNS_TOPIC_ARN on Events+SNS lambda")
    sns_topic_arn = ensure_sns_topic_and_policy(sns, account_id, SNS_TOPIC_NAME)
    log(f"SNS Topic ARN: {sns_topic_arn}")
    ensure_email_subscription(sns, sns_topic_arn, ADMIN_EMAIL)

    # -------------------------
    # Env vars on Lambdas (merge-safe)
    # -------------------------
    merge_lambda_env_vars(client_lambda, EVENTS_SNS_LAMBDA_NAME, {
        "SNS_TOPIC_ARN": sns_topic_arn,
        "EVENTS_TABLE_NAME": EVENTS_TABLE_NAME,
        "FRAMES_BUCKET": FRAMES_BUCKET,
        "FRAMES_PREFIX": FRAMES_PREFIX,
    })

    log("Updating Detector env vars (merge-safe): set RENDER_LAMBDA_NAME/EVENTS_LAMBDA_NAME and remove SNS_TOPIC_ARN")
    merge_lambda_env_vars(client_lambda, DETECTOR_LAMBDA_NAME, {
        "RENDER_LAMBDA_NAME": RENDER_LAMBDA_NAME,
        "EVENTS_LAMBDA_NAME": EVENTS_SNS_LAMBDA_NAME,
        "SNS_TOPIC_ARN": None,
        "FRAMES_BUCKET": FRAMES_BUCKET,
        "FRAMES_PREFIX": FRAMES_PREFIX,
        "EVENTS_TABLE_NAME": EVENTS_TABLE_NAME,
    })

    merge_lambda_env_vars(client_lambda, RENDER_LAMBDA_NAME, {
        "FRAMES_BUCKET": FRAMES_BUCKET,
        "FRAMES_PREFIX": FRAMES_PREFIX,
        "EVENTS_TABLE_NAME": EVENTS_TABLE_NAME,
    })

    # -------------------------
    # Detector Function URL
    # -------------------------
    log(f"Ensuring Detector has Function URL: {DETECTOR_LAMBDA_NAME}")
    detector_url = ensure_function_url(client_lambda, DETECTOR_LAMBDA_NAME, ALLOWED_ORIGIN)

    ensure_lambda_permission_invoke(
        client_lambda,
        fn_name=DETECTOR_LAMBDA_NAME,
        statement_id=f"{STACK_PREFIX}DetectorFunctionUrlPublic",
        principal="*",
        action="lambda:InvokeFunctionUrl",
        function_url_auth_type="NONE",
    )
    log(f"Detector Function URL: {detector_url}")

    # -------------------------
    # Events API Lambda (/events)
    # -------------------------
    log(f"Ensuring Events API Lambda exists (used by /events): {EVENTS_API_LAMBDA_NAME} (prefer api_handler.zip)")
    events_api_arn = get_lambda_arn(client_lambda, EVENTS_API_LAMBDA_NAME)

    if events_api_arn is None:
        api_real = os.path.join(here, "api_handler.zip")
        if os.path.exists(api_real):
            log(f"Using zip: {api_real}")
            zbytes = b64_zip_bytes(api_real)
        else:
            log(f"Missing {api_real} -> creating placeholder /events API zip")
            with tempfile.TemporaryDirectory() as td:
                zpath = os.path.join(td, "events_api.zip")
                make_events_api_placeholder_zip(zpath)
                zbytes = b64_zip_bytes(zpath)

        events_api_arn = ensure_lambda_function(
            client_lambda, EVENTS_API_LAMBDA_NAME, role_arn, DEFAULT_PY_RUNTIME, "lambda_function.lambda_handler",
            zbytes, timeout=15, memory=256, env_vars={}
        )
    else:
        log(f"Events API Lambda exists: {events_api_arn}")

    merge_lambda_env_vars(client_lambda, EVENTS_API_LAMBDA_NAME, {
        "EVENTS_TABLE_NAME": EVENTS_TABLE_NAME,
        "FRAMES_BUCKET": FRAMES_BUCKET,
        "FRAMES_PREFIX": FRAMES_PREFIX,
    })

    # -------------------------
    # HTTP API (API Gateway v2)
    # -------------------------
    log(f"Ensuring HTTP API: {API_NAME}")
    api_id = ensure_http_api(apigw, API_NAME)
    ensure_default_stage(apigw, api_id)

    api = apigw.get_api(ApiId=api_id)
    api_endpoint = api["ApiEndpoint"]
    log(f"API Endpoint: {api_endpoint}")

    log("Creating/Ensuring integrations")
    login_integration_id = ensure_integration(apigw, api_id, login_arn)
    events_integration_id = ensure_integration(apigw, api_id, events_api_arn)

    source_arn = f"arn:aws:execute-api:{REGION}:{account_id}:{api_id}/*/*/*"
    log("Ensuring Lambda invoke permission for API Gateway (Login + Events API)")

    ensure_lambda_permission_invoke(
        client_lambda,
        fn_name=LOGIN_LAMBDA_NAME,
        statement_id=f"{STACK_PREFIX}ApiGwInvokeLogin",
        principal="apigateway.amazonaws.com",
        action="lambda:InvokeFunction",
        source_arn=source_arn,
    )

    ensure_lambda_permission_invoke(
        client_lambda,
        fn_name=EVENTS_API_LAMBDA_NAME,
        statement_id=f"{STACK_PREFIX}ApiGwInvokeEventsApi",
        principal="apigateway.amazonaws.com",
        action="lambda:InvokeFunction",
        source_arn=source_arn,
    )

    log("Ensuring JWT Authorizer for /events")
    authz_id = ensure_jwt_authorizer(apigw, api_id, client_id, issuer)

    log("Creating routes...")
    ensure_route(apigw, api_id, "POST /auth/login", login_integration_id, "NONE", None)
    ensure_route(apigw, api_id, "POST /auth/complete-password", login_integration_id, "NONE", None)
    ensure_route(apigw, api_id, "GET /auth/me", login_integration_id, "NONE", None)
    ensure_route(apigw, api_id, "POST /auth/logout", login_integration_id, "NONE", None)

    ensure_route(apigw, api_id, "GET /events", events_integration_id, "JWT", authz_id)
    ensure_route(apigw, api_id, "PATCH /events", events_integration_id, "JWT", authz_id)
    ensure_route(apigw, api_id, "OPTIONS /{proxy+}", login_integration_id, "NONE", None)

    deploy_api(apigw, api_id)

    # -------------------------
    # Frontend: S3 Static Website Hosting
    # -------------------------
    log("Deploying frontend to S3 Static Website Hosting...")
    frontend_cors_origins = _parse_csv_env(FRONTEND_CORS_ALLOWED_ORIGINS)
    if not frontend_cors_origins:
        frontend_cors_origins = ["*"]

    website_url = deploy_frontend_static_website(
        s3,
        bucket=FRONTEND_BUCKET_NAME,
        region=REGION,
        local_dir=FRONTEND_DIR,
        cors_allowed_origins=frontend_cors_origins,
        api_base_url=api_endpoint,
        detector_lambda_url=detector_url,
    )

    website_origin = website_url.rstrip("/")

    log(f"Updating Login Lambda ALLOWED_ORIGIN -> {website_origin}")
    merge_lambda_env_vars(client_lambda, LOGIN_LAMBDA_NAME, {
        "ALLOWED_ORIGIN": website_origin,
    })

    # -------------------------
    # Summary
    # -------------------------
    print("\n==============================")
    print("DONE")
    print(f"Region:        {REGION}")
    print(f"User Pool:     {USER_POOL_NAME} ({user_pool_id})")
    print(f"App Client:    {APP_CLIENT_NAME} (ClientId: {client_id})")
    print(f"Groups:        {GROUP_ADMINS}, {GROUP_LIFEGUARDS}")
    print("Users:")
    print(f"  Admin:       {ADMIN_EMAIL}  (temp pass: {TEMP_PASSWORD}) -> {GROUP_ADMINS}")
    print(f"  Lifeguard:   {GUARD_EMAIL}  (temp pass: {TEMP_PASSWORD}) -> {GROUP_LIFEGUARDS}")
    print(f"Login Lambda:  {LOGIN_LAMBDA_NAME}")
    print(f"Events API Lambda (/events): {EVENTS_API_LAMBDA_NAME}")
    print("Split Lambdas:")
    print(f"  Detector:    {DETECTOR_LAMBDA_NAME} (handler: {DETECTOR_HANDLER}, mem: {DETECTOR_MEMORY}MB)")
    print(f"  Render:      {RENDER_LAMBDA_NAME}   (handler: {RENDER_HANDLER}, mem: {RENDER_MEMORY}MB)")
    print(f"  Events+SNS:  {EVENTS_SNS_LAMBDA_NAME} (handler: {EVENTS_SNS_HANDLER}, mem: {EVENTS_SNS_MEMORY}MB)")
    print(f"Detector Function URL: {detector_url}")
    print(f"SNS Topic:     {SNS_TOPIC_NAME}")
    print(f"SNS Topic ARN: {sns_topic_arn}")
    print(f"HTTP API:      {API_NAME} (ApiId: {api_id})")
    print(f"API Base URL:  {api_endpoint}\n")
    print(f"Frontend S3 Website: {website_url}")
    print("==============================\n")


if __name__ == "__main__":
    try:
        main()
    except subprocess.CalledProcessError as e:
        die(f"Command failed: {e}")
    except ClientError as e:
        die(f"AWS error: {e}")
    except Exception as e:
        die(f"Unexpected error: {e}")
